[build-system]
requires = ["setuptools>=64.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "data_migrations"
version = "0.0.1"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "pyspark==4.0.1",
    "python-dotenv==1.2.1",
    "tomli==2.3.0",
    "Faker==38.2.0",
]

[project.optional-dependencies]
tables = [
    "pyarrow==22.0.0",
    "pyarrow-stubs==20.0.0.20251215",
    "adbc-driver-manager==1.9.0",
    "adbc-driver-postgresql==1.9.0",
    "pandas==2.3.3",
    "pyiceberg[sql-sqlite]==0.10.0",
]
sql = ["psycopg2-binary==2.9.11", "SQLAlchemy==2.0.44"]
integration = [
    "clickhouse-connect==0.10.0",
    "boto3==1.42.39",
    "kafka-python==2.3.0",
    # "s3fs==2025.12.0",
]

[project.scripts]
library = "library.lib:main"
star = "data_warehouse.star:main"
data_vault = "data_warehouse.data_vault:main"
partitions = "data_warehouse.partitions:main"
clickhouse = "clickhouse:main"
s3 = "s3_upload:main"
spark = "spark_upload:main"
kafka = "kafka_pipeline:main"

[tool.setuptools]
package-dir = { "" = "src" }

[tool.setuptools.package-data]
airflow_pipeline = ["sql/*.sql", "data/*"]
kafka_pipeline = ["sql/*.sql", "data/*"]
spark_upload = ["sql/*.sql", "data/*"]
s3_upload = ["sql/*.sql"]

[tool.setuptools.packages.find]
where = ["src"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = ["-v", "--strict-markers", "--tb=short"]
