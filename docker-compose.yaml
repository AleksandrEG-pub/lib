name: it_one

services:
  airflow:
    image: apache/airflow:3.1.7rc1-python3.13
    command: standalone
    environment:
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_ALL_ADMINS: True
      AIRFLOW_CONN_BACKEND: http://airflow-pipeline:8080
    ports:
      - "10460:8080"
    volumes:
      - ./src/airflow_dags:/opt/airflow/dags
    
  airflow-pipeline:
    build:
      context: .
      dockerfile: Dockerfile.spark_app
      target: airflow_pipeline
    ports:
      - "10459:8080"
      - "5678:5678"
    env_file:
      - path: ./env/spark.env
        required: true
      - path: ./env/database_docker.env
        required: true

  kafka:
    image: apache/kafka:3.7.0
    env_file:
      - path: ./env/kafka_broker.env
        required: true

  kafka-pipeline:
    build:
      context: .
      dockerfile: Dockerfile.spark_app
      target: kafka_pipeline
    ports:
      - "5678:5678"
    env_file:
      - path: ./env/kafka.env
        required: true
      - path: ./env/spark.env
        required: true
      - path: ./env/database_docker.env
        required: true

  seaweedfs:
    image: chrislusf/seaweedfs:4.07
    ports:
      - "10456:8333"
      - "10457:23646"
    environment:
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: key
    command: "server -s3 -dir=/data"

  spark-upload:
    build:
      context: .
      dockerfile: Dockerfile.spark_app
      target: spark_upload
    env_file:
      - path: ./env/database_docker.env
        required: true
      - path: ./env/spark.env
        required: true
      - path: ./env/s3_docker.env
        required: true
    depends_on:
      - spark-master
      - spark-worker
    ports:
      - "5678:5678"

  spark-master:
    # 7077:7077 api
    # 10458:8080 ui
    build:  
      context: .
      dockerfile: Dockerfile.spark_app
      target: spark_base
    command: >
      sh -c "
        /opt/spark/sbin/start-master.sh;
        tail -F /opt/spark/logs/spark-spark-org.apache.spark.deploy.master*.out
      "
    ports:
      - "10458:8080"
      
  spark-worker:
    build:
      context: .
      dockerfile: Dockerfile.spark_app
      target: spark_base
    depends_on:
      - spark-master
    command: >
      sh -c "
        /opt/spark/sbin/start-worker.sh spark://spark-master:7077;
        tail -F /opt/spark/logs/spark-spark-org.apache.spark.deploy.worker*.out
      "
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=1G

  database-lib:
    image: postgres:17.5
    restart: unless-stopped
    ports:
      - "10452:5432"
    env_file:
      - ./env/database.env

  clickhouse:
    image: clickhouse:25.11.3.54-jammy
    restart: unless-stopped
    env_file:
      - path: ./env/clickhouse.env
        required: true
    ports:
      - "10453:8123"
      - "10454:9000"

  grafana:
    image: grafana/grafana:12.3.0-18765596677-ubuntu
    restart: unless-stopped
    env_file:
      - ./grafana/.env
    ports:
      - "10455:3000"
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/var/lib/grafana/dashboards
      - ./grafana/grafana.ini:/usr/local/etc/grafana/grafana.ini
    depends_on:
      - clickhouse
